\documentclass[11pt]{article}
\usepackage{fullpage,amsthm,amsfonts,amssymb,epsfig,amsmath,times,amsthm,graphicx}


\begin{document}

\title{Project Writeup: Pacman Capture the Flag}
\author{Jared Jensen, Chris Gradwohl \\ Team: Organic Stupidity}
\date{CMPS 140 Winter 2017 \\ University of California, Santa Cruz}
\maketitle


\begin{abstract}
	Lorem ipsum dolor sit amet, consectetur adipiscing elit,
	sed do eiusmod tempor incididunt ut labore et dolore magna
	aliqua. Ut enim ad minim veniam, quis nostrud exercitation
	ullamco laboris nisi ut aliquip ex ea commodo consequat.
	Duis aute irure dolor in reprehenderit in voluptate velit
	esse cillum dolore eu fugiat nulla pariatur. Excepteur sint
	occaecat cupidatat non proident, sunt in culpa qui officia
	deserunt mollit anim id est laborum.
\end{abstract}


\section{Approximate Q-learning Agent}

\subsection{Fundamental Problem and Obstacles}
Our initial approach involved understanding capture the flag and determining
what type of agent would fit into this problem. We realized right away that
the game involved several strategies based on several features. It seemed like
if we could define a proper set of features then a q-learning agent would work
well in capture the flag. \

Our first goal was to create an overall offensive and defensive agent for capture the flag. We
chose to model the capture the flag problem as a set of features that the q-learning agent
could learn weights for. Initially we settled on \textit{number of food pellets, number of defending food pellets, score,
closest food pellets, opponent distance} as our set of features. These features seemed
to be working well, and once we correctly saved the weights so that all 4 agents could work together
they were performing better than expected. \

\subsection{Agent Evaluation}
In our initial implementation of the q-learning agent we did not generalize enough. We realized
that when training the agent we only did so on one map. So given the feature, the agent
was able to learn how to perform well given one map, but when it played on a random seed, it
failed miserably. \

The first problem was that we told the agent(via the feature set) to move randomly at first, and to listen to
enemy movements, while it was rewarded for obtaining food pellets.
After training on one map it was able to make its way out of the maze and into
enemy territory to obtain food pellets and receive its reward. However given a new random map,
its previous feature weights were meaningless and it was unable to get out of its initial position. \

The second problem was that we implemented the weights as a global parameter, meaning that
the agent would import and use the weights from a previous episode. This did not generalize to
a new random map.\

\subsection{Refined Approximate Q-learning Agent}
To overcome the agents mobility issues we implemented an A-star feature, that allowed
the agent to learn distances to the nearest pellet. This gave the agent the ability to
move into enemy territory in a more direct manner. \

Secondly we removed the global weights and added a weight counter to the agent object.
We did ten training games and manually added the weights the agent learned from
training. We updated the agent to have a training mode so that when training
it starts from scratch.\

With these refinements we submitted our agent to our first tournament.\

\subsection{Notable Results}
By far the most interesting(and startling) result of our q-learning agent was that
given this set of features, the agent learned how to avoid ghosts. We never actually
explicitly programed a feature that gave the agent a negative reward for being eaten.
The feature for enemy position combined with the discount rate, allowed our agent to
realize that when the \textit{opponent distance} was really low, it performed poorly.
When our agent would die it would have to go all the way to the starting part of the map, and through
learning episodes and \textit{opponent distance} feature, it found that is was bad to be
near the ghosts. In other words it obtained more rewards when it avoided
ghosts, and therefore chose actions to run away! \

This was a really cool and interesting result of feature learning. It is a great example of
choosing correct features that results in positive behavior.

\section{Max Flow Defense}

















\end{document}
